{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple situation\n",
    "# just one kind of cell\n",
    "\n",
    "# x -- RNAseq, y -- ATACseq\n",
    "# x1,y2 true data, x2,y1 false data\n",
    "# x1 -> y1 G:generator, Dy:discriminator\n",
    "# y2 -> x1 F:generator, Dx:discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scRAP(object):\n",
    "    \"\"\"\n",
    "    a simple situation version\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,m,n):\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        \n",
    "    @staticmethod\n",
    "    def lrelu(x,alpha=0.2):\n",
    "        with tf.variable_scope('leakyRelu'):\n",
    "            return tf.maximum(x,alpha*x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def trans_to_atac(x):\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.floor(tf.add(x,0.5))\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def trans_to_rnasq(x):\n",
    "        x = tf.floor(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "    # generator G\n",
    "    def G(self,Z,dim_Z):\n",
    "        with tf.variable_scope(\"G\",reuse=tf.AUTO_REUSE):\n",
    "            dim_1 = 2*dim_Z\n",
    "            dim_2 = 2*self._n\n",
    "            dim_3 = self._n\n",
    "            \n",
    "            W1 = tf.get_variable(\"G_W1\",shape=[dim_Z,dim_1],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W2 = tf.get_variable(\"G_W2\",shape=[dim_1,dim_2],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W3 = tf.get_variable(\"G_W3\",shape=[dim_2,dim_3],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            B1 = tf.get_variable(\"G_B1\",shape=[dim_1],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B2 = tf.get_variable(\"G_B2\",shape=[dim_2],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B3 = tf.get_variable(\"G_B3\",shape=[dim_3],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            \n",
    "            fc1 = self.lrelu(tf.add(tf.matmul(Z,W1),B1))\n",
    "            fc2 = self.lrelu(tf.add(tf.matmul(fc1,W2),B2))\n",
    "            fc3 = tf.nn.sigmoid(tf.add(tf.matmul(fc2,W3),B3))\n",
    "            #fc3 = self.trans_to_atac(fc3)\n",
    "            return fc3\n",
    "        \n",
    "    # generator F\n",
    "    def F(self,Z,dim_Z):\n",
    "        with tf.variable_scope(\"F\",reuse=tf.AUTO_REUSE):\n",
    "            dim_1 = 2*dim_Z\n",
    "            dim_2 = 2*self._m\n",
    "            dim_3 = self._m\n",
    "            \n",
    "            W1 = tf.get_variable(\"F_W1\",shape=[dim_Z,dim_1],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W2 = tf.get_variable(\"F_W2\",shape=[dim_1,dim_2],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W3 = tf.get_variable(\"F_W3\",shape=[dim_2,dim_3],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            B1 = tf.get_variable(\"F_B1\",shape=[dim_1],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B2 = tf.get_variable(\"F_B2\",shape=[dim_2],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B3 = tf.get_variable(\"F_B3\",shape=[dim_3],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            \n",
    "            fc1 = self.lrelu(tf.add(tf.matmul(Z,W1),B1))\n",
    "            fc2 = tf.nn.sigmoid(tf.add(tf.matmul(fc1,W2),B2))\n",
    "            fc3 = tf.add(tf.matmul(fc2,W3),B3)\n",
    "            #fc3 = self.trans_to_rnasq(fc3)\n",
    "            return fc3\n",
    "    \n",
    "    # discriminator Dy\n",
    "    def Dy(self,Y):\n",
    "        with tf.variable_scope(\"Dy\",reuse=tf.AUTO_REUSE):\n",
    "            dim_1 = self._n//2\n",
    "            dim_2 = self._n//4\n",
    "            dim_3 = 1\n",
    "            \n",
    "            W1 = tf.get_variable(\"Dy_W1\",shape=[self._n,dim_1],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W2 = tf.get_variable(\"Dy_W2\",shape=[dim_1,dim_2],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W3 = tf.get_variable(\"Dy_W3\",shape=[dim_2,dim_3],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            B1 = tf.get_variable(\"Dy_B1\",shape=[dim_1],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B2 = tf.get_variable(\"Dy_B2\",shape=[dim_2],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B3 = tf.get_variable(\"Dy_B3\",shape=[dim_3],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            \n",
    "            fc1 = self.lrelu(tf.add(tf.matmul(Y,W1),B1))\n",
    "            fc2 = tf.nn.sigmoid(tf.add(tf.matmul(fc1,W2),B2))\n",
    "            fc3 = tf.add(tf.matmul(fc2,W3),B3)\n",
    "            return fc3,tf.nn.sigmoid(fc3)\n",
    "        \n",
    "     # discriminator Dx\n",
    "    def Dx(self,X):\n",
    "        with tf.variable_scope(\"Dy\",reuse=tf.AUTO_REUSE):\n",
    "            dim_1 = self._m//2\n",
    "            dim_2 = self._m//4\n",
    "            dim_3 = 1\n",
    "            \n",
    "            W1 = tf.get_variable(\"Dx_W1\",shape=[self._m,dim_1],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W2 = tf.get_variable(\"Dx_W2\",shape=[dim_1,dim_2],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            W3 = tf.get_variable(\"Dx_W3\",shape=[dim_2,dim_3],\n",
    "                                 dtype=tf.float32,initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "            B1 = tf.get_variable(\"Dx_B1\",shape=[dim_1],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B2 = tf.get_variable(\"Dx_B2\",shape=[dim_2],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            B3 = tf.get_variable(\"Dx_B3\",shape=[dim_3],dtype=tf.float32,initializer=tf.constant_initializer())\n",
    "            \n",
    "            fc1 = self.lrelu(tf.add(tf.matmul(X,W1),B1))\n",
    "            fc2 = tf.nn.sigmoid(tf.add(tf.matmul(fc1,W2),B2))\n",
    "            fc3 = tf.add(tf.matmul(fc2,W3),B3)\n",
    "            return fc3,tf.nn.sigmoid(fc3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossc,flossc,dylossc,dxlossc = train(data,m,n,0.01,20,0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(glossc,'k-')\n",
    "plt.show()\n",
    "plt.plot(dylossc,'r--')\n",
    "plt.show()\n",
    "plt.plot(flossc,'k-')\n",
    "plt.show()\n",
    "plt.plot(dxlossc,'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data,batch_size):\n",
    "    input_queue = tf.train.slice_input_producer([data], num_epochs=1, shuffle=True, capacity=32 ) \n",
    "    batch = tf.train.batch(input_queue, batch_size=batch_size, num_threads=1, capacity=32, allow_smaller_final_batch=False)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,m,n,learning_rate,batch_size,lamda):\n",
    "    data_x = data[\"x\"]\n",
    "    data_y = data[\"y\"]\n",
    "    func = scRAP(m,n)\n",
    "\n",
    "    with tf.variable_scope(\"placeholder\",reuse=tf.AUTO_REUSE):\n",
    "#         Z_x = tf.placeholder(tf.float32,[None,m])\n",
    "#         Z_y = tf.placeholder(tf.float32,[None,n])\n",
    "        X = tf.placeholder(tf.float32,[None,m])\n",
    "        Y = tf.placeholder(tf.float32,[None,n])\n",
    "        \n",
    "    with tf.variable_scope(\"cycGAN\",reuse=tf.AUTO_REUSE):\n",
    "        false_y = func.G(X,m)\n",
    "        false_x = func.F(Y,n)\n",
    "        false_y_x = func.F(false_y,n)\n",
    "        false_x_y = func.G(false_x,m)\n",
    "        false_Dy_logit, false_Dy_prob = func.Dy(false_y)\n",
    "        real_Dy_logit, real_Dy_prob = func.Dy(Y)\n",
    "        false_Dx_logit, false_Dx_prob = func.Dx(false_x)\n",
    "        real_Dx_logit, real_Dx_prob = func.Dx(X)\n",
    "        \n",
    "    with tf.variable_scope(\"gan_loss\",reuse=tf.AUTO_REUSE):\n",
    "        G_loss = lamda*tf.reduce_mean(tf.square(false_y_x-X))+lamda*tf.reduce_mean(\n",
    "            tf.square(false_x_y-Y))+tf.reduce_mean(tf.square(false_Dy_prob-1))\n",
    "        F_loss = lamda*tf.reduce_mean(tf.square(false_y_x-X))+lamda*tf.reduce_mean(\n",
    "            tf.square(false_x_y-Y))+tf.reduce_mean(tf.square(false_Dx_prob-1))\n",
    "        Dy_loss = tf.reduce_mean(tf.square(real_Dy_prob-1))+tf.reduce_mean(tf.square(false_Dy_prob))\n",
    "        Dx_loss = tf.reduce_mean(tf.square(real_Dx_prob-1))+tf.reduce_mean(tf.square(false_Dx_prob))\n",
    "        \n",
    "    with tf.variable_scope(\"train\",reuse=tf.AUTO_REUSE):\n",
    "        G_step = tf.train.AdamOptimizer(learning_rate).minimize(G_loss)\n",
    "        F_step = tf.train.AdamOptimizer(learning_rate).minimize(F_loss)\n",
    "        Dy_step = tf.train.AdamOptimizer(learning_rate).minimize(Dy_loss)\n",
    "        Dx_step = tf.train.AdamOptimizer(learning_rate).minimize(Dx_loss)\n",
    "        \n",
    "    init_op = tf.global_variables_initializer()\n",
    "   # x_batch = get_batch(data[\"x\"],batch_size)\n",
    "   # y_batch = get_batch(data[\"y\"],batch_size)\n",
    "    #pdb.set_trace()\n",
    "    glossc = []\n",
    "    flossc = []\n",
    "    dylossc = []\n",
    "    dxlossc = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for i in range(1000):\n",
    "            data_x, data_y = data[\"x\"],data[\"y\"]#sess.run([x_batch,y_batch])\n",
    "            _, gloss = sess.run([G_step,G_loss],feed_dict={X:data_x,Y:data_y})\n",
    "            _, floss = sess.run([F_step,F_loss],feed_dict={X:data_x,Y:data_y})\n",
    "            _, dyloss = sess.run([Dy_step,Dy_loss],feed_dict={X:data_x,Y:data_y})\n",
    "            _, dxloss = sess.run([Dx_step,Dx_loss],feed_dict={X:data_x,Y:data_y})\n",
    "            if i % 10 == 0:\n",
    "                glossc.append(gloss)\n",
    "                flossc.append(floss)\n",
    "                dylossc.append(dyloss)\n",
    "                dxlossc.append(dxloss)\n",
    "               # print(\"Epoch:{4},Gloss:{0},Floss:{1},Dyloss:{2},Dxloss:{3}\".format(gloss,floss,dyloss,dxloss,i))\n",
    "    return glossc,flossc,dylossc,dxlossc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "m = 16\n",
    "n = 32\n",
    "num_sample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation data\n",
    "sigma = np.exp(np.random.normal(size=[num_sample,m]))\n",
    "rnasq = np.random.poisson(sigma)\n",
    "A = 0.01*np.random.normal(size=[m,n])\n",
    "atac = 0.5*np.random.normal(size=[num_sample,n])+np.matmul(rnasq,A)\n",
    "atac[atac>0.5] = 1\n",
    "atac[atac<=0.5] = 0\n",
    "data = {\"x\":rnasq, \"y\":atac}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round([3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
